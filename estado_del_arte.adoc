// Configuración github
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

ifndef::included[]
:numbered:
:toc:
:toclevels: 5
:lang: es
:encoding: utf8
:sectnumlevels: 5
:toc-title: Índice
:toc-placement: manual
:stem: latexmath
:imagesdir: _images
:table-caption: Tabla
:stem: latexmath
toc::[]

== Estado del arte

endif::[]


=== Educational DataMining

==== Introducción

Ya se ha definido en este documento EDM como la aplicación de las técnicas tradicionales de Data Mining al mundo de la educación.
La International Educational Data Mining Society, que nace en 2011 con el objetivo de fomentar la colaboración y el desarrollo científico de esta nueva disciplina, la define en su web (www.educationalData Mining.org) como "una disciplina emergente cuyo objetivo es el desarrollo de métodos para explorar los tipos de datos particulares que provienen de entornos educativos y usar esos métodos para comprender mejor a los estudiantes y los entornos en los que aprenden".

Es decir, el objetivo de EDM es entresacar el  conocimiento escondido detrás de grandes cantidades de datos, en este caso procedentes de las herramientas educativas, usando técnicas cómo el aprendizaje automático, la inteligencia artificial o la estadística.

EDM es una disciplina moderadamente reciente. El primer evento significativo de EDM se organizó en 2005 (hace poco más de diez años): un taller en la
Association for the Advancement of Artificial Intelligence o AAAI. La primera conferencia internacional sobre la materia tuvo lugar en 2008. Y la publicación del primer número del Journal of Educational Data Mining se produce en 2009.

Posiblemente debido a su _juventud_ los límites de actuación de EDM no están definidos con absoluta precisión. De hecho es un área que supone un cruce de caminos entre varias disciplinas, de entre las cuales cabe destacer las tres siguientes: ciencias de la computación, educación y estadísticas (Romero y Ventura, 2013).

Existen también muchos puntos en común con Learning Analytics. Baker e Inventado (2014) destacan como diferencia que EDM está más orientado al análisis automático de datos mientras que LA está más enfocado como herramienta de apoyo para que un ser humano valore la información resumida.

==== Proceso

El proceso de Data Mining es aplicable a cualquier sector o actividad que genera gran cantidad de datos.
En EDM, como ya se ha dicho, se aplicará este proceso dentro del marco educativo.

CRISP-DM (del inglés Cross Industry Standard Process for Data Mining) es un estándar abierto que define el modelo de proceso que se seguirá para proyectos de Data Mining.

.Fases de CRISP-DM (Kenneth Jensen)
image::CRISP-DM_Process_Diagram.png[]

Las fases de este modelo se detallan a continuación:

. Definición del Problema:
+
El proceso de Data Mining empieza entendiendo el problema. Es un ejercicio conjunto que involucra a los expertos del dominio de conocimiento y de Data Mining, de forma que se pueda trasladar el objetivo a la definición del problema del data mining.
+
. Exploración de los datos:
+
En esta etapa del proceso se analiza el significado de los metadatos. Se recogen, se describen y se exploran los datos. También es la fase en la que se analizan los posibles déficits de calidad de los datos. Para esta fase, se usan herramientas estadísticas.
. Preparación de los datos:
+
En esta fase se recopilan, se limpian y se formatean los datos, es decir, se construye el modelo de datos creando los atributos necesarios. Los datos se preparan para la herramienta de la siguiente fase.
. Modelado
+
En esta fase se aplican diferentes funciones de minería de datos. Algunas de las funciones de minería pueden usar los datos con un tipo específico de datos, en esta fase se deben establecer los diferentes modelos.
Esta fase es iterativa y debe ser repetida en sucesivas ocasiones, cambiando y modificando parámetros hasta obtener el modelo óptimo.
. Evaluación
+
En esta fase se evalúa el modelo. Si el modelo no satisface las necesidades se debe volver a la fase de modelado, revisando los parámetros, hasta conseguir la respuesta deseada.
. Despliegue
+
Los expertos en Data Mining exportan los resultados a herramientas desde las que podrán ser explotados (como tablas de bases de datos relacionales u hojas de cálculo) para la aplicación del modelo correspondiente.


==== Clasificación de técnicas

Baker (2010) afirma que existen varias formas de clasificar los métodos o técnicas dentro de EDM.
Una de ellas es la que proponen Romero y Ventura (2007):

* Visualización y estadísticas
* Web mining:
** descubrimiento de estructuras, clasificación y detección de valores atípicos.
** minería de reglas de asociación y de patrones secuenciales.
** Minería de textos.

Esta forma está demasiado orientada al mundo web, por lo que Baker (2010) propone otra clasificación de la que aquí se extrae lo más importante:

* Predicción.
** Regresión.
** Clasificación.
* Clustering.
* Relationship Mining.
** Correlation Mining.
** Causal Data Mining.
** ARM o Association Rule Mining.
** Sequential Pattern Mining.
* Distillation of data for human judgment.
* Discovery with models.

De esta última clasificación se verán a continuación los puntos más destacables.

===== Predicción

Romero y Ventura (2013) se refieren a esta categoría como aquella cuyo objetivo es predecir un aspecto concreto de un conjunto de datos (la variable a predecir) a partir de otras variables de ese mismo conjunto de datsos.

Se puede emplear tanto para predecir el futuro (¿qué alumno va a aprobar?) como el presente (¿qué alumno está padeciendo acoso o está desmotivado?).

La utilidad de estas predicciones puede ser múltiple.
Algunos ejemplos de esta utilidad pueden ser:

* Mejorar el contenido educativo.
Puesto que sabemos que el alumno se aburre o no está motivado (e incluso en qué momentos ocurre) podemos mejorar el material didáctico.
* Toma de decisiones automáticas. Si el alumno no está entendiendo cierto contenido se le puede ofrecer automáticamente ciertas ayudas dirigidas.
* Informar a los actores pertinentes del entorno educativo (alumnos, profesores, tutores, padres, directores, etc.) de ciertas tendencias detectadas para que puedan revertirlas o alentarlas.

Los tipos de métodos predictivos más destacables son la regresión y la clasificación.
Ambos se verán a continuación.

====== Regresión

Se emplea este método cuando lo que se quiere predecir es un valor numérico (por ejemplo: la nota final de 0 a 10 o el número de asignaturas que aprobará un alumno).

Para construir un modelo de regresión se parte de un conjunto de datos de entrenamiento.
Es decir, datos para los cuales conocemos el valor del atributo o variable que se quiere predecir.

Junto a ese valor que se  quiere predecir existe otra serie de parámetros o variables que serán susceptibles de ser usados para la predicción.

El resultado de aplicar este método define qué parámetros y combinados de qué manera sirven para predecir la variable de interés para el estudio.

La regresión se divide en regresión lineal y en árboles de regresión.

La regresión lineal es la más típica.
En ella la variable que se quiere predecir se expresa como una función lineal de los parámetros que hacen de predictores.

Por ejemplo:

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2
++++++++++++++++++++++++++++++++++++++++++++

Si la regresión lineal es múltiple considera múltiples variables independientes.
Esto tendrá sentido cuando todas las variables incluidas tengan un efecto aditivo, es decir, aporten información para determinar el valor final de la variable a predecir.

Estas relaciones lineales no siempre responden a los problemas actuales con múltiples datos y con varias interacciones que hacen que aumente el número de parámetros.

Los árboles de regresión no son más que árboles de decisión donde el valor final es una variable de tipo numérico.

El objetivo es predecir los valores de una variable basándose en reglas de decisión sencillas a partir de los datos origen. Los diferentes algoritmos van segmentando los datos en grupos más homogéneos hasta llegar al último nodo, donde no se pueden realizar nuevas particiones. Cada división en la que se separan los datos representan una hoja del árbol.

Dos ejemplos de algoritmos de este tipo serían: M5 y RepTree.

.Pseudocódigo de ejemplo de un árbol de regresión RepTree (no lineal)
=====
 If X>10
   Y = 22
 Else If Z<-5
   Y = 1
 Else Y = 11
=====

.Pseudocódigo de ejemplo de un árbol de regresión M5 ( lineal)
====
 If X>55
   Y = 11A + 4B
 Else If X < 1
   Y = 22A – 5B
 Else Y = 9A + 8B + 1,2C
====

====== Métricas para regresión

Es necesario utilizar unas medidas comunes que sirvan para poder comparar unos métodos con otros. Para las técnicas de regresión las más importantes son estas:

.Coeficiente de correlación de Pearson

Es una medida de la posible relación lineal entre dos variables.
Es decir, si se consideran dos variables A y B, esta métrica establece si, al incrementar el valor de A, se produce un incremento lineal en el valor de B.

Se define el coeficiente de correlación de Pearson para dos variables (X, Y) como:

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
\rho_{X,Y} = \frac{\sigma_X_Y}{\sigma_X\sigma_Y}
++++++++++++++++++++++++++++++++++++++++++++

Donde:

* latexmath:[\sigma_{XY}]: es la covarianza de X, Y.
* latexmath:[\sigma_X]: es la desviación típica de la variable X.
* latexmath:[\sigma_Y]: es la desviación típica de la variable Y.


Análisis de los distintos valores:

* r = 1: la correlación es perfecta.
* 0 < r < 1: existe una correlación positiva.
* r = 0: no existe relación lineal entre las dos variables consideradas.
* r = -1: existe una correlación perfecta pero inversa.
* -1 < r < 0: existe una correlación negativa.

En general, para los valores intermedios, su interpretación depende del campo en el que se esté aplicando.
Para EDM, por ejemplo, una correlación de 0,3 se considera buena.

También se emplea el valor r², es decir, el coeficiente de correlación al cuadrado.
Es una medida de qué porcentaje de la variación está explicada por el modelo.

.MAE/RMSE

El valor absoluto medio (MAE por sus siglas en inglés: Mean Absolute Error) se calcula como la media del valor absoluto de la diferencia entre el valor predicho y el valor real.

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
MAE = \frac {\sum_{i=1}^n |y_i - x_i|}{n}
++++++++++++++++++++++++++++++++++++++++++++

Indica la media de la desviación del error predicho con respecto al real.

RMSE (Root Mean Squared Error o Raíz cuadrada de la media del error al cuadrado) es un indicador que se interpresta de forma parecida al anterior pero que penaliza más las desviaciones más grandes.

// ====== Information Criteria
// Criterio de Información Bayesiano
//
// Se intenta llegar a un compromiso entre el número de parámetros utilizados en el
// modelo y la calidad del mismo. Cuantos más parámetros se consideran para el
// modelo, normalmente mejora la predicción del valor de salida pero el riesgo de
// esto es que el modelo se ajuste demasiado a los datos de entrada (overfitting)
// no siendo un  modelo que se pueda generalizar para otros datos. Para evitar ese
// sobreajuste, se _penaliza_ cuantas más variables se empleen para el modelo.
//
// Akaike’s Information Criterion (Akaike, 1974)
// Es un método similar al anterior.
//

====== Clasificación

En esta categoría la variable de la que quiere predecir su valor no es de tipo numérica, sino categórica.
Por ejemplo: ¿a qué universidad irá un alumno?, ¿dejará un alumno el colegio antes de tiempo o no?.

De forma similar a lo que ocurría con la regresión, la combinación de ciertos valores de ciertos atributos serán capaces de predecir un valor concreto.

Hay muchísimos algoritmos de clasificación. Algunos ejemplos:

*  Step regression
*  Logistic regression
*  árboles de decisión (J48/C4.5)
*  Jrip decision rules
*  clasificadores K* Instance-based

A continuación se verán algunos ejemplos.

//
// ====== Step regression
// WARNING: No es stepwise regression!!!!
//
// Para clasificación binaria. En primer lugar se busca una función linear para
// hacer regresión lineal con un umbral arbitrario. Se seleccionan parámetros, se
// asigna un peso a cada parámetro y se calcula el valor numérico correspondiente.
// Si dicho valor supera el umbral establecido, se asigna a la variable a predecir
// el 1, de otro modo el 0
//
// ====== Regresión logística
// Es un algoritmo que, al igual que el anterio, también está pensado para
// clasificación con dos categorías o binaria.
// https://es.wikipedia.org/wiki/Regresi%C3%B3n_log%C3%ADstica
//
// Función lineal en función de las variables independientes. Con cuyo valor
// podremos calcular la probabilidad de que se dé la variable dependiente.
//
// Ejemplo:
//
// [latexmath]
// ++++++++++++++++++++++++++++++++++++++++++++
// m = \beta_1*X_0 + \beta_2*X_1 ...
// ++++++++++++++++++++++++++++++++++++++++++++
//
// [latexmath]
// ++++++++++++++++++++++++++++++++++++++++++++
// p(m) = \frac{1}{1+e^{-m}}
// ++++++++++++++++++++++++++++++++++++++++++++
//
// Es un algoritmo conservador
//
// WARNING: ¿Qué significa conservador?(¿qué significa, lo explica más adelabnte?)
//

.Árboles de decision (J48 o C4.5 o C5.0)

C4.5 Es un algoritmo desarrollado por Ross Quinlan y que supuso una evolución a partir de otro algoritmo del mismo autor: ID3.

J48 es el nombre que recibe la implementación en Java y de código abierto del algoritmo C4.5.
Esta _reimplementación_ se realizó expresamente para la herramienta Weka.

El mismo Ross Quinlan ha trabajado en una nueva versión del algoritmo, C5.0, la cual presenta significativas mejoras con respecto a su predecesora (por ejemplo en cuanto a velocidad o memoria).
//
// Tiene en cuentra la interacción entre variables (a dferencia de los dos
// algoritmos anteriores)

Esta familia de algoritmos puede tratar tanto con variables de tipo numérico como con categorías.

// Tiene sentido cuando hay interacciones entre variables o cuando se puede llegar
// al mismo resultado desde distintos puntos.
//

En <<j48_tree_en_weka>> se puede ver un árbol creado con el algoritmo J48 en Weka a partir de datos de ejemplo de la propia herramienta de Data Mining.

.Ejemplo de árbol J48 en Weka
[[j48_tree_en_weka]]
image::j48_tree.png[]

Este algoritmo es uno de los más conocidos y empleados dentro de Data Mining (Wu et al., 2008).

// ====== Decision Rules (Jrip y PART)
// Reglas de tipo *SI x ENTONCES* y que se comprueban en orden.
//
// Las diferencias entre los distintos algoritmos están más en cómo se generan y
// seleccionan las variables.
//
// Dos de los ejemplos más conocidos de estos algoritmos son JRIP y PART
//
// Para crear reglas desde decision trees:
//
// . Crear árbol de decisión
// . Si existe un camino que merezca la pena seguir, se pasa al paso 3, si no,
// al paso 6
// . Tomar el mejor camino de la raíz hasta el extremo y hacer de ese camino
// una regla.
// . Quitar todos los datos del dataset que se correspondan con el camino
// recién considerado.
// . Ir al paso 1
// . Considerar el resto de datos.
// . Encontrar el valor mayoritario de los datos restantes
// . Hacer una regla del tipo OTHERWISE (es decir, PARA EL RESTO DE CASOS).
//
// Relativamente conservador
//
// Modelos fácilmente interpretables.
//
// Buenos cuando se dan interacciones en varios niveles.
//

.Clasificadores K* Instance-based
Predice el valor de la variable correspondiente que se quiere predecir teniendo en cuenta otros puntos de datos vecinos. Los puntos más cercanos tendrán mayor peso en el análisis.
//
// Este algoritmo es recomendable cuando:
//
// * los datos sean muy divergentes. Muchos procesos distintos pueden conducir
// al mismo resultado.
// * Sea muy complicado aplicar reglas generales.
//
// Es, en muchos casos, un plan B para cuando todo lo demás no funciona.
//

En Baker et al. (2012) se asegura que ha sido un algoritmo muy útil para detectar emociones a partir de ficheros de log.

Su principal inconveniente es que es necesario tener todos los datos para poder construir este modelo.
//
// ====== Bagged stumps
// WARNING: [Lo que hay es del curso de EDM pero soy incapaz de encontrar nada más]
//
// Está relacionado con decision trees. Muchos árboles con sólo la primera feature.
// Relativamente conservador. Random Forest es una variante parecida.
// Todos los clasificadores considerados hasta ahora son conservadores:
//
// *  Encuentran modelos simples
// *  No hacen overfit
//
// Estos algoritmos parecen ser más apropiados para EDM que otros algoritmos menos
// conservadores.
//
// Algunos algoritmos menos conservadores a continuación:
//
// ====== Support Vector Machines
// Útil para minería de texto y para datos de sensores. Crea modelos muy
// sofisticados. Sin embargo no es tan útil para el resto de datos que conciernen a
// EDM.
//
// ====== Algoritmos genéticos
//
// ====== Neural Networks
// Modelos muy complejos. Perceptrones
// Caso de estudio para clasificación

====== Métricas para clasificación

Al igual que ocurría con la regresión, es importante no solamente saber el valor final de la variable que se quiere predecir sino también su calidad, es decir, cómo de cierto es su valor.
Por ejemplo, se tendrá más información si se dice que "hay un 50% de probabilidades de que el alumno vaya a  abandonar el colegio" que si sólamente se dice que el modelo predictivo predice que el alumno va a abandonar el colegio.


Algunas de las métricas asociadas a la clasificación se detallan a continuación.

// .Detector confidence:
// Esto es útil, por ejemplo, para decidir cómo será la intervención sobre un
// alumno. También considerando cuál es el coste de aplicar una intervención
// incorrecta y cuál el de no intervenir cuando sí es neceario.
//
// El uso de detector confidence puede ser para análisis de Discovery with models.
//
// No siempre se puede tener este valor. Hay algunos clasificadores que no ofrecen
// esta estimación. Por ejemplo, step regression ofrece pseudo confidence.
//
// Algunos algoritmos dan este dato de forma directa y rotunda. En otros casos es
// necesario realizar cálculos a partir de la salida de los mismos. Por ejemplo en
// un árbol es común que para cada nodo último aparezca el número de elementos que
// para esa nodo están clasificados de las distintas maneras posibles.
//
// .Confidence
// Aunque no siempre se puede usar, si está disponible siempre conviene
// utilizar esta métrica.
//

.Exactitud o accuracy

Se puede definir como el número de predicciones correctas del total de predicciones realizadas.

No siempre es un buen medidor de la calidad de las predicciones.
Si, por ejemplo,  se tiene un curso cuyo porcentaje de aprobados es del 80%; si el modelo de predicción consiste en seleccionar siempre "aprobado" como resultado (lo que se denomina clasificador 0-R), la exactitud de nuestro modelo será del 80% pero, sin embargo, en realidad no es un muy buen predictor.

.Coeficiente Kappa de Cohen
Es una métrica que compara la exactitud esperada con la observada.


[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
k = \frac{(Pa-Pe)}{1-Pe}
++++++++++++++++++++++++++++++++++++++++++++

Siendo:

* Pa: la proporción de acuerdos entre la realidad y la predicción.
* Pe: la proporción de acuerdos esperada.

Interpretación de Kappa:

* Kappa = 0: el clasificador es igual de bueno que el azar.
* Kappa = 1: el predictor es perfecto.
* Kappa = -1: las predicciones son perfectamente opuestas a la realidad.
* Kappa < 0: en este caso el modelo es peor que el azar.
* Entre 0 y 1: son los valores más típicos.
Se suelen admitir como válidos valores entre 0.3 y 0.5 para considerar un modelo mejor que el azar.

Comparar Kappa entre dos conjuntos de datos es complicado.
Sólo tiene sentido compararlos si las distintas categorías en que se quiere clasificar son  proporcionales en su núnero de elementos.
//
// Ejemplo con dos categorías (Expected agreement = Expected acuerdo para categoría
// 1 + expected para categoría2, siendo EA1 = freq cat1*freqcat1 clasificada)

.Curvas ROC
Están pensadas para sistemas binarios de VERDADERO o FALSO.
Por ejemplo, es una métrica muy empleada para análisis de métodos diagnósticos en medicina.

Se usa en sistemas que, más que darnos el resultado de la clasificación, nos dan un valor a partir del cual podemos establecer un umbral para marcar el dato como VERDADERO o FALSO.

Las curvas ROC (acrónimo de Receiver Operating Characteristic o Característica Operativa del Receptor) son una representación del porcentaje de FALSOS POSITIVOS  (valores clasificados como positivos cuando realmente no lo son).

En el eje Y se sitúa la tasa de VERDADEROS POSITIVOS  (también llamado *sensibilidad*) frente al eje X, donde se representa la tasa de FALSOS POSITIVOS, que es equivalente a 1-*especificidad*.


[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
sensibilidad = \frac{VP}{VP + FN}
++++++++++++++++++++++++++++++++++++++++++++

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
especificidad = \frac{VN}{VN+FP}
++++++++++++++++++++++++++++++++++++++++++++

// ialbors: sensibilidad: de entre todos los que son positivos, cuántos se han clasificado como tal.
// ialbors: especificidad: de entre todos los que son negativos, cuántos se han clasificado como negativos.
// tasa de falsos positivos: de todos los que son negativos, cuántos se han clasificado como positivos

La curva se dibuja calculando ese par de valores modificando el criterio de decisión para determinar que un punto es VERDADERO o FALSO.

.Ejemplo de curvas ROC (Liang, Liu y Sun, 2017).
[[curvas_roc]]
image::roc_curves.png[]

En <<curvas_roc>> se puede ver un ejemplo de cuatro curvas ROC para sendos clasificadores.
Cuanto mejor es la curva ROC más se sitúa hacia el vértice superior izquierdo.
En este caso la curva roja se corresponde con el mejor clasificador según su curva ROC y la curva azul se corresponde con el peor. 

//
// Cutoff sería el umbral de decisión para decidir
//
// Ejemplos de curvas ROC correctas y malas
//
// ====== A’
// Define la probabilidad de que, si al modelo se le propone un caso de cada
// categoría, si sabrá identificar cuál se corresponde con cuál.
//
// Para tres o más categorías existen métodos para calcularlo pero la semántica
// cambia.
//
// Sirve para comparar dos modelos o para comparar un modelo frente al puro azar.
//
// Relacionado con ROC.
//
// También llamada AUC (Area under Curve) porque se puede calcular como el área
// bajo la curva ROC.
//
// Es matemáticamente equivalente a la estadística Wilcoxon.
//
// Comparación con Kappa:
//
// *  Es más difícil de calcular
// *  Sólo funciona con dos categorías (sin añadir excesiva complejidad)
// *  Es posible comparar valores entre datasets distintos. A’=0,6 siempre es
// mejor que A’=0,5
// *  Fácil de interpretar estadísticamente.
// *  Suele tener valores más altos que Kappa.
// *  Tiene en cuenta confidence
//

.Precisión
Es la probabilidad de que un punto clasificado como VERDADERO sea realmente VERDADERO.

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
Precision = \frac{TP}{TP + FP}
++++++++++++++++++++++++++++++++++++++++++++

Siendo:

* TP: TRUE POSITIVE o Verdadero Positivo (se ha marcado correctamente como VERDADERO).
* FP: FALSE POSITIVE o Falso Positivo (se ha clasificado incorrectamente como VERDADERO).

.Exhaustividad
La probabilidad de que un punto que es VERDADERO sea clasificado como VERDADERO.

[latexmath]
++++
Recall = \frac{TP}{TP + FN}
++++

// ===== Validación cruzada y sobreajuste
// El sobreajuste se da cuando el modelo no sólo se ajusta a la señal sino también
// al ruido.
//
// Ejemplo
//
// Para reducir el sobreajuste se emplean diferentes métodos:
//
// *  Usar modelos más simples:
// ** con menos variables (ver BiC o AiC)
// ** funciones menos complejas (MDL)
//
// Todos los modelos sufren de este problema en mayor o menor medida.
//
// Lo importante es que nuestro modelo se puedan adaptar a nuevos contextos.
//
// ====== Método 1: método de retencion (o training set vs test set)
// Un método es separar los datos de entrada en dos grupos diferenciados (grupo de
// entrenamiento y grupo de test). De este modo el modelo se prueba contra el grupo
// de test que contiene elementos que el modelo no había visto antes.
//
// Si bien este método es muy rápido de procesar, su principal problema es que
// pueden darse muchas variaciones en los resultados dependiendo de cómo se
// establezca esa división entre datos de entranamiento y datos para evaluar.
//
// ====== Método 2: Validación cruzada
// Con este método se separa los datos en varios conjuntos disjuntos con el mismo
// número de elementos. Se realiza el entrenamiento sobre todos los conjuntos menos
// uno de ellos, sobre el que se hará la evaluación.
//
// .K-Fold
// se dividen los datos en K grupos del mismo tamaño. Es más rápido. Es típico usar
// K = 10.
//
// .Leave-out-one
// En este caso cada dato es un forma un subconjunto. Es más estable y precisa
// pero, obviamente, existe un coste alto para procesarlo.
//
// .Flat cross validation
// Cada punto tiene la misma probabilidad de encontrarse en cada fold
//
// .Stratified cross validation:
// Se elige cada subconjunto para que las variables estén igualmente representadas.
//
// .Student level cross-validation:
// Los grupos son seleccionados de modo que la información de un estudiante en
// concreto no puede estar representada en dos grupos.
//
// De este modo se puede generalizar el modelo para nuevos estudi	antes.
//
// En EDM se considera el mínimo nivel para hacer cross-validation
//
// Otros niveles empleados para Cross-validation:
// *  Lección/Contenido
// *  Escuela
// *  Demografia (población urbana/rural, raza, sexo)
// *  Software package
//
// La pregunta clave es para qué tipo de nuevos datos se quiere usar el modelo:
// para nuevos estudiantes, nuevas escuelas, nueva población, nuevo software. Se
// hace cross-validation para ese valor.
//
// ===== Validez del modelo
//
// .Generalizable
// Si un modelo sigue siendo predictivo con nueva información que el modelo no haya
// visto nunca.
//
// .Validez ecológica:
// Establece si un modelo funcionará en un entorno real y no únicamente en un
// entorno de laboratorio (donde se pueden dar ciertos presupuestos o se puede
// ejercer cierto control que no existirá en un entorno real).
//
// .Construct Validity:
// Si el modelo mide lo que realmente quería medir.
//
// .Validez predictivo:
// Si el modelo puede predecir no solamente el presente sino también el futuro.
//
// .Substantive validity:
// Si el modelo tiene relevancia. Predice algo importante o de interés.
//
// .Content validity
// Si cubre todo el dominio que pretende cubrir.
//
// .Conclusion validity:
// Si las conclusiones están justificadas basándose en la evidencia

===== Clustering
El descubrimiento de estructuras o _clustering_ tiene por objeto la segmentación de los datos.
Se agrupan los elementos de un conjunto mayor en subconjuntos, llamados _clusters_, que estén lo suficientemente relacionados sin tener ninguna idea preconcebida sobre cómo serán esos subconjuntos.

No hay una variable resultado de la que quiera conocer su valor final; solamente se definen esos clusters en los que los datos se agrupan por la similitud interna.
Es decir, que si se realiza la agrupación correctamente, los datos dentro de un mismo cluster son más parecidos entre sí que respecto a los datos de otros clusters.

Los métodos de clustering aplicados a EDM más usados son:

* K-Means
* Expectation Maximization (EM)-Based Clustering
* Gaussian Mixture Models

El clustering es usado cuando se sabe relativamente poco del modelo. Al usarlo en entornos muy conocidos podemos llegar a conclusiones ya conocidas.


===== Minería de relaciones

El objetivo de estas técnicas es encontrar relaciones ocultas a simple vista entre las variables de un conjunto de datos.
//
// Dentro de esta clasificación existen subclasificaciones.

====== Association Rule Mining
Consiste en encontrar relaciones entre variables del tipo:

 IF A, B, C THEN X, Y, Z.

Se emplea para encontrar relaciones sorprendentes o inesperadas entre las variables.

El algoritmo más conocido y usado dentro de esta clasificación es el algoritmo Apriori (Agrawal, Imielinski y Swami, 1993).

Existen varias métricas que suelen usarse para comparar y valorar modelos creados a partir de ARM. Se definen a continuación las más importantes y empleadas.

.Support

Hay dos formas de definir este valor.
Se puede definir como una proporción: número de instancias que cumplen la regla generada (antecedente y consecuente) entre el número de instancias total.
O bien como un valor absoluto: el número total de instancias que cumplen la regla.

.Confidence

Se define como el número de instancias que cumplen la regla entre el número de instancias que cumplen el antecedente de la regla.

.Lift

Si se tiene una regla del tipo:
 A => B

El valor lift se calcula de esta forma:

[latexmath]
++++++++++++++++++++++++++++++++++++++++++++
Lift = \frac{Confidence(A->B)}{P(B)}
++++++++++++++++++++++++++++++++++++++++++++

Siendo:

* P(B): la probabilidad de B.

====== Correlation Mining
// http://www.eecs.umich.edu/eecs/pdfs/events/2711.pdf
// http://users.wpi.edu/~rsbaker/Encyclopedia%20Chapter%20Draft%20v10%20-fw.pdf
//
El objetivo es encontrar correlaciones lineales entre variables cuando el número de variables es muy alto.

El enfoque puede ser más reducido y así intentar
encontrar qué variables están correlacionadas con otra fija en la que se tiene
un interés especial o bien un enfoque más amplio en el que se quiera encontrar
asociaciones significativas entre dos variables cualquiera del conjunto de
variables sin ninguna limitación inicial.
//
// Bonferroni y FDR como correcciones.

====== Causal Data Mining
El objetivo es hallar relaciones causales entre variables.
// http://nugget.unisa.edu.au/Thuc/CausalAssRule.pdf

Es decir, no solamente debe existir una asociación entre dos variables o entre los valores de dos variables, sino que esa relación debe ser causal.
Por ejemplo, podría existir una asociación entre “suspender Matemáticas” y
“suspender Lengua” en un conjunto determinado de datos.
Que exista esa asociación no quiere decir que “suspender Matemáticas” sea causa de “suspender
Lengua” o viceversa.
En cambio podría existir una tercera variable -- “nivel académico previo”, por ejemplo -- que sí podría ser causa de las otras dos.

Una forma de estudiar las posible relaciones causales de forma experimental implica la
manipulación de los sujetos de estudio de modo que variando los valores de una variable
A se estudie si esa variación repercuta en el valor de otra variable B.
Las implicaciones éticas que conlleva este procedimiento, especialmente cuando el objeto de estudio es un ser humano (como es el caso de EDM), suponen un importante impedimento.
Por ejemplo si se obliga a un alumno a no atender en clase para comprobar cómo repercute eso
en sus notas.

Es por ello por lo que es importante ser capaces de obtener relaciones causales
únicamente a partir de datos ya recopilados.

====== Sequential Pattern mining

En esta categoría el objetivo es crear relaciones como las de ARM pero en la que se tiene en cuenta el factor tiempo.

Es decir el antecedente es anterior en el tiempo al consecuente. Por ejemplo:

 IF estudiante elige asignaturaA este año THEN escoge asignaturaB el año siguiente.

===== Distillation of data for human judgment
El objetivo es presentar la información de forma resumida, inteligible, visual y/o interactiva para ayudar en la toma de decisiones (Romero y Ventura, 2013).

Como se ve en la propia definición, estaría muy ligado a LA.

===== Discovery with models

El descubrimiento con modelos consiste en utilizar un modelo predictivo (creado a partir de algunas de las técnicas típicas de Data Mining) como un componente más para el análisis y creación de otro modelo predictivo (Romero y Ventura, 2013).

Generalmente se usa un modelo de predicción para una variable que posteriormente es usado con otro modelo de predicción donde esta variable pasa a ser una variable productora.

La otra técnica más frecuente es usar la predicción de una variable para posteriormente aplicar una técnica de minería de relaciones.

Es un término acuñado por Baker y Yacef (2009) que, según ellos mismos afirman, surge a partir de 2008/2009, por lo que todavía no tiene demasiado recorrido a pesar de que ya en 2010 suponía el 20% de los estudios publicados en EDM.


=== Moodle

include::moodle.adoc[]

=== Weka

include::weka.adoc[]
